{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f50f9ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"ner\"\n",
    "model_checkpoint = 'sentence-transformers/LaBSE'# \"setu4993/LaBSE\" # mBERT pre-trained from HuggingFace Hub\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8269acd9",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f15756e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (C:\\Users\\Bernard\\.cache\\huggingface\\datasets\\conll2003\\conll2003\\1.0.0\\40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "datasets = load_dataset(\"conll2003\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f17a1bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = datasets[\"train\"].features[f\"{task}_tags\"].feature.names\n",
    "\n",
    "label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a500495",
   "metadata": {},
   "source": [
    "### Processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6c62a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef96a583",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all_tokens = True\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True, max_length=400)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"{task}_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "141cf1d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4660f0350ac84adc9b5fc42a10dc065b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac64fd0905b24bf9989c656adba2c64f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3188a9ce03c46dba1fe6a07e01cd789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c65178f",
   "metadata": {},
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9922124c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at sentence-transformers/LaBSE were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at sentence-transformers/LaBSE and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9e7b91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=f\"test-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32a40be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af060124",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "491c124f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "        \"LOC-f1\": results['LOC'][\"f1\"],\n",
    "        \"LOC-precision\": results['LOC'][\"precision\"],\n",
    "        \"LOC-recall\": results['LOC'][\"recall\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6d283ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31c28d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add early stopping to trainer\n",
    "\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62fc4b56",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.44 GiB (GPU 0; 8.00 GiB total capacity; 4.95 GiB already allocated; 1.13 GiB free; 5.06 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, **kwargs)\u001b[0m\n\u001b[0;32m   1160\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1161\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1162\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1164\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\optim\\lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[1;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\optimization.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    334\u001b[0m                     \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"exp_avg\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m                     \u001b[1;31m# Exponential moving average of squared gradient values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m                     \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"exp_avg_sq\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexp_avg_sq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"exp_avg\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"exp_avg_sq\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.44 GiB (GPU 0; 8.00 GiB total capacity; 4.95 GiB already allocated; 1.13 GiB free; 5.06 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcb7141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf0e521",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c7cedd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "355ab27e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf3a4acf35ec42888d8541b972502044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/5.22M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9154dca07ea94483a1711cbd4689002f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/9.62M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67d27e1a24ae42179b270addfae0c0d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a4f7a1978254fd9a0b91296a587efb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6158553bc114480822975a87f49d297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/560 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c484c7aa60b04c4a965511ef4bc82b43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizerFast\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"setu4993/LaBSE\")\n",
    "model = BertModel.from_pretrained(\"setu4993/LaBSE\")\n",
    "model = model.eval()\n",
    "\n",
    "english_sentences = [\n",
    "    \"dog\",\n",
    "    \"Puppies are nice.\",\n",
    "    \"I enjoy taking long walks along the beach with my dog.\",\n",
    "]\n",
    "english_inputs = tokenizer(english_sentences, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    english_outputs = model(**english_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edc59699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78733d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0891, -0.3681,  0.0780,  ..., -0.6703, -1.0931, -0.8783],\n",
       "         [ 0.4157, -0.4741,  0.8040,  ..., -0.2907, -1.2842, -1.3074],\n",
       "         [-0.0891, -0.3681,  0.0780,  ..., -0.6703, -1.0931, -0.8783],\n",
       "         ...,\n",
       "         [ 0.2710, -0.1676,  0.5026,  ..., -0.0687, -0.8197, -0.9091],\n",
       "         [ 0.2820, -0.1816,  0.5116,  ...,  0.0051, -0.8328, -0.9173],\n",
       "         [ 0.2692, -0.1470,  0.5313,  ..., -0.0312, -0.8280, -0.9146]],\n",
       "\n",
       "        [[ 0.4134,  0.8301,  0.2401,  ..., -0.4857, -0.8877, -0.1453],\n",
       "         [ 1.0111, -0.0830,  1.0415,  ...,  0.1864, -1.3976, -0.7726],\n",
       "         [ 0.9079,  0.1531,  0.8516,  ...,  0.1026, -1.0914, -0.3634],\n",
       "         ...,\n",
       "         [ 1.0021,  0.2776,  0.5574,  ..., -0.0480, -0.8325, -0.2678],\n",
       "         [ 0.9893,  0.2820,  0.5472,  ...,  0.0370, -0.8336, -0.2561],\n",
       "         [ 0.9865,  0.3638,  0.5971,  ..., -0.0057, -0.8459, -0.2536]],\n",
       "\n",
       "        [[-0.0802,  0.6718,  0.5903,  ..., -1.2023, -1.1710,  0.2188],\n",
       "         [ 0.5892, -0.4631,  0.4525,  ..., -0.7194, -1.3563, -0.5075],\n",
       "         [ 0.6791, -0.8532,  0.3015,  ..., -0.9006, -1.3010, -0.5216],\n",
       "         ...,\n",
       "         [ 0.5542, -0.3070,  1.4933,  ..., -0.5979, -1.7458, -0.6805],\n",
       "         [ 0.6070, -0.4914,  0.6924,  ..., -0.7103, -1.3511, -0.2836],\n",
       "         [-0.0802,  0.6718,  0.5903,  ..., -1.2023, -1.1710,  0.2188]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c56d46fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-1.3473e-01, -2.1608e-02, -1.9249e-01, -4.0513e-01, -2.3961e-01,\n",
       "          2.7830e-01,  5.6244e-01,  3.7427e-02, -8.3857e-01, -7.1555e-01,\n",
       "         -6.4725e-02,  4.9802e-01,  7.6643e-01, -4.4483e-01, -2.5206e-01,\n",
       "         -9.1947e-01,  7.0459e-01, -5.7049e-01,  5.4599e-01,  3.7562e-01,\n",
       "         -1.7535e-01, -7.2055e-01,  3.4746e-01, -6.9417e-01, -6.6054e-01,\n",
       "         -1.3161e-01,  3.5984e-01,  1.3651e-01, -1.7165e-01, -2.2598e-01,\n",
       "         -3.8189e-01,  3.7355e-01, -5.1195e-01, -7.3582e-01, -6.6230e-01,\n",
       "         -2.2338e-01, -5.1664e-01,  3.2047e-01,  4.7127e-01, -8.6406e-01,\n",
       "          5.4163e-01, -4.6512e-01, -1.0731e-01,  1.8689e-02,  6.0123e-01,\n",
       "          5.7982e-01,  1.3521e-01,  1.2690e-01, -4.4128e-01, -4.7346e-01,\n",
       "         -3.9215e-01, -7.7843e-01, -8.2898e-01, -8.2672e-01, -8.5188e-01,\n",
       "          7.8737e-01, -9.6975e-01, -3.5543e-01, -4.7266e-01, -8.7708e-01,\n",
       "         -5.5298e-01, -7.2575e-01, -2.5778e-01,  1.1772e-01,  6.2019e-01,\n",
       "          6.4406e-01, -6.7005e-01, -7.0817e-01, -8.2079e-01, -1.3297e-02,\n",
       "          3.1506e-01, -8.9130e-01, -2.1687e-01, -9.4468e-02, -8.0399e-01,\n",
       "         -1.4122e-01, -2.0236e-01, -5.6104e-01, -2.3223e-01, -5.4861e-01,\n",
       "          3.2838e-01,  6.1451e-01,  1.3868e-01, -2.5499e-01, -4.9263e-01,\n",
       "         -8.6823e-01, -5.8445e-01, -8.2160e-01, -1.6036e-01, -8.4342e-01,\n",
       "         -1.0180e-01,  3.5105e-01,  1.7756e-01, -1.0496e-02, -1.0788e-01,\n",
       "         -3.2105e-01,  5.7926e-04,  2.0027e-01, -1.0779e-01, -5.6612e-01,\n",
       "         -4.1812e-01, -3.5739e-01, -5.6503e-02, -1.9262e-01,  5.5954e-02,\n",
       "         -6.7613e-01,  1.8410e-01, -1.3738e-01, -5.3287e-01,  7.2763e-01,\n",
       "          1.1054e-01, -6.1846e-01, -2.9575e-01, -6.0691e-01, -1.4162e-01,\n",
       "         -3.3295e-01,  1.7395e-01, -8.0105e-01, -3.7465e-01, -3.1631e-01,\n",
       "          6.2078e-02,  2.2949e-01, -4.4806e-01, -4.7522e-01, -6.3582e-01,\n",
       "          2.8903e-01, -3.5696e-01,  6.7033e-01, -3.8476e-02,  3.8874e-01,\n",
       "          4.8373e-01, -4.7819e-01, -8.1623e-01, -6.6823e-01, -3.0330e-01,\n",
       "          5.6349e-02, -5.5477e-01, -7.2599e-01, -5.6860e-01, -8.1646e-01,\n",
       "         -1.2424e-01, -3.8668e-01, -7.1414e-01, -3.9794e-01, -5.4681e-01,\n",
       "         -2.1683e-01,  3.5084e-02, -1.7080e-01, -5.4320e-01,  5.6904e-01,\n",
       "         -2.4557e-01, -1.5165e-01,  8.0144e-02,  7.1505e-02,  2.7102e-03,\n",
       "         -1.2578e-01, -2.6471e-01, -8.2710e-01, -4.6375e-02, -4.7540e-01,\n",
       "         -5.4722e-01,  2.4337e-01, -8.9144e-01,  3.8642e-01, -4.8139e-02,\n",
       "          7.3377e-01, -5.4954e-01,  1.8968e-01, -2.2545e-01,  4.6170e-02,\n",
       "         -3.1101e-02,  1.0156e-01, -4.6952e-01, -4.8023e-01, -4.6942e-01,\n",
       "          5.5793e-01, -7.3134e-01, -5.7020e-01, -7.0121e-01,  3.6047e-01,\n",
       "         -5.5565e-01, -3.6453e-01, -7.3033e-02, -2.3522e-01, -7.7640e-01,\n",
       "          1.4997e-02, -1.6713e-01, -8.1318e-01,  6.5233e-01,  5.7148e-01,\n",
       "         -4.0295e-01, -2.3437e-01,  6.0562e-01,  3.6194e-02,  7.5604e-02,\n",
       "         -3.9411e-01, -4.3396e-01, -1.1729e-01, -6.4825e-01,  2.3725e-01,\n",
       "         -5.8446e-01, -4.0424e-01, -8.2168e-01, -3.6431e-01, -5.7478e-01,\n",
       "         -3.2880e-01, -2.3418e-01, -7.4155e-01,  3.1378e-03, -3.4068e-01,\n",
       "         -9.2193e-01, -4.7237e-01,  3.7445e-01, -2.2364e-01,  2.0136e-01,\n",
       "          6.6172e-01, -9.9590e-01,  7.7206e-01,  8.5838e-01, -6.9665e-01,\n",
       "         -6.1418e-01, -4.7346e-01,  2.2127e-01, -1.5614e-01, -1.8600e-01,\n",
       "         -2.5812e-01,  1.1902e-01,  1.5309e-01, -4.4469e-01,  1.6682e-01,\n",
       "         -5.7560e-01, -4.2373e-01,  5.9064e-02,  2.7965e-01, -2.2035e-01,\n",
       "         -1.3907e-01,  4.9777e-01, -1.2160e-01,  3.5240e-01, -5.1137e-01,\n",
       "          2.7408e-02,  2.1478e-01,  5.5673e-01, -8.2357e-01, -5.4578e-01,\n",
       "         -2.4376e-01, -1.9948e-01,  2.6049e-01, -5.3730e-01,  3.1813e-01,\n",
       "         -9.0542e-01,  3.2361e-01, -8.8096e-01,  4.2870e-01, -4.8296e-03,\n",
       "         -6.4154e-01, -7.7922e-01, -8.1421e-01, -3.4421e-01,  6.6564e-01,\n",
       "         -3.5136e-01, -2.2467e-01, -1.3348e-01,  8.4949e-01,  5.8187e-01,\n",
       "         -7.6548e-01,  1.2017e-01,  6.9696e-01, -4.4965e-02,  5.6523e-01,\n",
       "         -6.2102e-01, -8.0269e-01, -1.0751e-01,  6.0186e-01, -4.6632e-01,\n",
       "         -2.9636e-01,  6.5837e-02, -1.6543e-01, -5.4557e-01, -1.2821e-01,\n",
       "         -8.5393e-01,  6.4635e-01,  2.0535e-01, -8.7665e-01, -2.5906e-01,\n",
       "         -6.1486e-01,  6.8383e-01, -9.6007e-01, -4.6492e-01,  5.2415e-02,\n",
       "         -7.7180e-01, -8.1077e-01, -8.3822e-01, -3.0414e-01, -1.1690e-01,\n",
       "         -2.7009e-01, -3.4257e-01, -2.1802e-01, -6.9165e-01,  4.6061e-01,\n",
       "         -3.7620e-01, -4.7784e-01,  3.1448e-02, -5.7204e-01, -7.3225e-02,\n",
       "         -8.1655e-02,  6.9820e-01, -9.2950e-01, -7.6105e-01, -1.3375e-01,\n",
       "         -4.7359e-01,  1.6745e-01,  1.9905e-02,  1.7173e-01, -9.9024e-02,\n",
       "         -7.2595e-01, -2.3554e-01,  5.1455e-01,  1.5288e-01, -5.6279e-01,\n",
       "          7.4850e-01, -7.5395e-01, -3.3634e-01, -6.5232e-01,  9.3825e-02,\n",
       "         -1.3634e-01,  2.8516e-02,  2.2938e-02,  3.5825e-01, -3.1814e-01,\n",
       "         -1.4401e-01,  7.1780e-01,  3.0494e-01, -8.0280e-01, -2.9014e-02,\n",
       "         -4.9235e-01,  1.9735e-01, -5.7738e-02, -1.4255e-01,  7.8785e-02,\n",
       "          1.0733e-01, -6.9919e-01, -4.3286e-01, -4.0549e-01,  7.3478e-01,\n",
       "         -3.6018e-01,  1.8769e-01,  3.9794e-01,  4.4034e-01,  4.7232e-01,\n",
       "         -5.9118e-02, -3.6175e-01, -2.0562e-01, -8.7759e-01, -8.5231e-01,\n",
       "          3.4868e-01, -5.0100e-01,  3.4027e-01, -4.7026e-01,  3.0015e-01,\n",
       "         -8.4274e-01, -8.6644e-01, -3.7160e-01,  1.2392e-01, -8.4267e-01,\n",
       "         -6.4377e-01, -2.9101e-01, -6.1924e-01, -2.1074e-02,  5.0817e-01,\n",
       "          4.7419e-01, -2.2505e-01, -2.1836e-01,  6.1080e-01, -6.9889e-02,\n",
       "         -7.4755e-02, -5.3791e-01,  3.4437e-01, -8.3986e-01,  5.9348e-02,\n",
       "         -6.1647e-01,  1.4115e-01,  1.2369e-01, -6.2380e-01, -8.5127e-01,\n",
       "         -7.3952e-01, -1.5168e-01, -5.2897e-01, -8.7680e-01, -3.1417e-01,\n",
       "          4.2885e-01, -1.8356e-01,  3.2117e-02,  4.2157e-01,  4.0840e-01,\n",
       "          3.1918e-01,  3.6681e-01, -5.7531e-01, -7.2612e-01, -3.4060e-01,\n",
       "         -3.4121e-02, -3.5261e-01, -3.1416e-01, -4.1613e-01,  2.9720e-02,\n",
       "         -8.8784e-01,  4.1792e-01, -5.8060e-01, -1.4091e-01,  3.5824e-01,\n",
       "          2.9430e-01, -1.6058e-01,  3.1787e-01,  1.9086e-01,  4.8799e-01,\n",
       "         -2.6472e-01, -4.1045e-01, -5.6426e-01,  2.1358e-01, -3.1049e-02,\n",
       "         -5.0861e-01, -8.6334e-01,  2.0817e-01, -1.2613e-01, -8.5786e-01,\n",
       "          2.6685e-02,  2.8978e-01,  4.1898e-01,  1.9356e-01, -8.2365e-02,\n",
       "         -8.4550e-02, -3.5966e-01,  2.5805e-01,  3.4561e-02,  7.6045e-02,\n",
       "          2.0379e-01, -4.4277e-01,  1.3207e-01, -4.3536e-01, -4.9682e-01,\n",
       "         -4.7322e-01, -3.3311e-02, -1.8091e-01,  1.3864e-01, -7.4963e-01,\n",
       "          1.5071e-01, -5.4728e-01, -3.0747e-01, -2.3951e-01,  3.1289e-01,\n",
       "         -7.1866e-02, -8.2591e-01,  3.4610e-01,  6.1635e-01,  2.3056e-01,\n",
       "          2.2534e-01, -4.1465e-01, -3.5768e-01, -7.2611e-01, -7.9053e-01,\n",
       "          5.1264e-01,  5.3567e-01, -3.9706e-01, -3.0476e-02, -8.4411e-01,\n",
       "          6.3077e-01, -6.2769e-01, -4.1540e-01, -6.4475e-01,  4.1694e-01,\n",
       "         -1.0146e-01,  5.2903e-01, -8.6370e-01, -6.5164e-01, -3.3909e-01,\n",
       "         -2.4705e-01,  2.0302e-01,  4.9294e-01, -5.5774e-01, -8.3414e-01,\n",
       "         -4.7620e-01, -4.7558e-01, -5.2616e-01, -6.2492e-01,  5.5221e-01,\n",
       "         -5.5637e-01, -6.5823e-01, -5.7688e-01,  5.7166e-01,  1.0515e-03,\n",
       "         -3.7597e-01,  3.0096e-01, -9.1237e-01,  7.8680e-01, -7.6587e-01,\n",
       "         -1.0113e-01, -2.9184e-02, -3.5400e-01, -8.8568e-01, -9.2161e-01,\n",
       "          5.9437e-01,  3.1247e-01, -7.0754e-01, -1.6864e-02,  5.8289e-02,\n",
       "          2.7001e-02, -1.5301e-01, -4.9299e-01,  3.4437e-01, -8.3073e-01,\n",
       "          6.8085e-02,  7.1939e-01, -5.7022e-03, -3.5380e-01,  6.8359e-02,\n",
       "         -4.5998e-02, -1.9395e-01, -3.8870e-01,  3.3300e-01,  1.7586e-01,\n",
       "         -3.1971e-01, -1.0668e-01,  1.1454e-01, -9.1115e-01, -5.1967e-01,\n",
       "         -1.3125e-02,  1.7590e-01, -6.6584e-01, -4.8398e-01, -6.5820e-01,\n",
       "          5.9717e-01, -1.9571e-01,  4.2038e-02,  4.2447e-01, -5.7073e-01,\n",
       "         -6.0390e-01, -1.0863e-01, -8.7194e-02, -3.7396e-01,  4.6049e-01,\n",
       "         -3.5875e-01,  7.4151e-01, -2.9551e-01, -6.2898e-01,  1.3487e-01,\n",
       "          1.6636e-01,  2.6434e-01,  2.6548e-01, -5.3311e-01, -8.0669e-01,\n",
       "         -2.1188e-01,  6.8869e-02, -4.8568e-01, -1.7810e-01, -5.1439e-01,\n",
       "         -2.8721e-01, -1.7222e-01, -1.7947e-01, -4.1831e-02, -6.1242e-01,\n",
       "         -8.9088e-02, -2.1597e-01, -3.6411e-01, -1.7867e-01, -2.3764e-01,\n",
       "         -9.8838e-02, -7.4244e-01,  3.5433e-01, -1.2175e-01, -4.4483e-01,\n",
       "         -5.5260e-01,  4.8876e-01, -1.5944e-01, -3.6336e-01,  6.8806e-01,\n",
       "          4.7244e-01,  4.8800e-01,  2.3180e-01, -4.1834e-01,  4.8005e-01,\n",
       "         -8.5809e-01, -5.0250e-01, -1.4927e-01, -3.5970e-01, -1.2391e-01,\n",
       "         -1.7685e-01, -7.8475e-02, -6.4442e-01,  1.2293e-01, -5.2767e-02,\n",
       "         -8.1819e-01,  5.4561e-01, -3.1352e-01, -5.8760e-01, -7.1415e-01,\n",
       "         -7.2991e-01,  3.1986e-01,  4.3717e-01,  5.3428e-01,  5.2722e-01,\n",
       "         -2.4282e-01, -3.9034e-01, -6.2946e-01, -6.2791e-01, -4.3953e-01,\n",
       "         -5.6144e-01,  2.0105e-01, -2.9447e-01, -5.3974e-01, -3.9650e-01,\n",
       "         -1.9072e-01, -3.0331e-01, -7.3303e-02, -1.4807e-01, -2.8741e-01,\n",
       "         -2.1115e-01, -7.1484e-01, -9.8202e-02, -1.9279e-01, -2.7608e-01,\n",
       "          1.6016e-01, -2.2061e-01,  3.4865e-01,  4.3824e-01, -1.9247e-01,\n",
       "         -5.8380e-01,  1.1774e-01, -1.3211e-02, -2.2758e-01,  2.5274e-02,\n",
       "          5.5966e-01, -4.7712e-01, -2.8450e-01, -6.9475e-01,  5.7219e-01,\n",
       "          5.6642e-02, -8.3544e-02, -1.5272e-01,  3.5955e-02, -6.3962e-01,\n",
       "         -8.5744e-01, -7.0347e-01, -9.6156e-01, -3.1273e-02, -3.8265e-02,\n",
       "          1.7390e-01, -1.5306e-01, -5.4390e-01,  5.0535e-01,  1.0643e-01,\n",
       "         -6.7799e-01,  2.1995e-01,  7.4655e-01, -7.6428e-01,  3.7574e-01,\n",
       "          1.0143e-01, -5.3759e-01, -3.5102e-01,  2.6652e-01,  8.3441e-01,\n",
       "         -1.0163e-01, -5.6274e-02, -6.7725e-01, -9.0629e-01, -7.1329e-01,\n",
       "         -5.6597e-01,  3.5250e-01,  1.4719e-02, -2.8052e-01, -6.2275e-01,\n",
       "          2.8581e-01, -4.5386e-01, -4.7784e-01, -3.2885e-01,  7.2471e-03,\n",
       "          5.2116e-01, -1.6273e-01, -8.2567e-01,  6.7746e-02, -7.6102e-01,\n",
       "          4.1909e-02, -5.3159e-01, -8.2577e-01,  4.5500e-01,  4.4960e-01,\n",
       "         -7.5387e-01, -2.7311e-01, -5.0316e-01, -8.3407e-01, -4.4498e-01,\n",
       "         -7.3175e-02, -6.5839e-01,  3.8104e-01, -2.5780e-01, -1.7126e-01,\n",
       "         -2.4681e-01, -3.2443e-01, -2.3675e-01,  4.9444e-01,  2.8253e-01,\n",
       "         -5.6328e-01, -5.5750e-01, -5.5607e-01, -2.2635e-01, -4.2224e-01,\n",
       "         -4.1826e-01, -1.7583e-01, -5.3396e-01, -5.9994e-01, -5.4130e-01,\n",
       "         -6.3790e-01, -6.8145e-02, -3.1236e-04, -1.3072e-01, -8.1669e-01,\n",
       "         -1.8076e-01, -6.9851e-01, -6.3352e-03, -3.4855e-01,  4.4768e-01,\n",
       "         -7.8669e-01,  1.7123e-01,  1.6598e-01,  4.6358e-01, -5.9801e-01,\n",
       "         -1.9576e-01, -4.0102e-01,  9.3314e-02, -2.6749e-01, -5.5831e-01,\n",
       "         -1.7380e-01,  4.3116e-01, -8.3976e-03,  9.8723e-02, -1.2187e-01,\n",
       "         -1.3501e-01, -6.3674e-01, -2.6797e-01, -6.3439e-01, -5.9847e-01,\n",
       "         -3.0406e-01, -4.2491e-01, -2.6226e-01,  1.2090e-01, -6.8866e-02,\n",
       "         -7.1816e-01, -4.4785e-01, -6.6398e-01, -6.8865e-02,  1.8718e-02,\n",
       "         -1.7778e-01, -2.0643e-01, -7.4253e-01,  3.9526e-01,  3.6904e-02,\n",
       "         -5.7660e-01, -6.3015e-01, -2.2474e-02,  3.3329e-01, -5.7125e-02,\n",
       "          2.6104e-01, -1.1032e-01, -2.8860e-01, -1.1556e-01, -5.3560e-01,\n",
       "          6.4466e-01, -1.6928e-01, -4.1565e-01]),\n",
       " tensor([-0.6227, -0.1059, -0.6587,  0.3611, -0.6386,  0.4731,  0.5737, -0.1348,\n",
       "         -0.3105, -0.7640,  0.1842,  0.1257,  0.3710,  0.4153, -0.7581, -0.3037,\n",
       "          0.4838,  0.3551, -0.3351,  0.1982, -0.0942, -0.3151, -0.0537, -0.1517,\n",
       "         -0.3438,  0.4376, -0.6898, -0.0341, -0.5051, -0.1328, -0.0486,  0.2174,\n",
       "          0.0870, -0.7042, -0.2648, -0.0112, -0.7749, -0.6935,  0.3848,  0.1095,\n",
       "          0.6681, -0.3862, -0.6868,  0.0289, -0.0970, -0.1720, -0.8967,  0.2731,\n",
       "          0.3557, -0.1951,  0.2753, -0.9637, -0.4740, -0.5933, -0.7604, -0.3487,\n",
       "         -0.5518, -0.7449, -0.6354,  0.3220, -0.0116, -0.2523, -0.4106,  0.0210,\n",
       "          0.5095,  0.4464, -0.4661,  0.4753,  0.2432,  0.3194, -0.1083, -0.2575,\n",
       "          0.3200, -0.2648,  0.1078,  0.4312, -0.0572, -0.3484, -0.2707, -0.0854,\n",
       "         -0.1154,  0.8358,  0.5397, -0.4920, -0.6986, -0.4427, -0.5795, -0.4878,\n",
       "          0.1316, -0.7822, -0.1393,  0.6570, -0.4963,  0.1993,  0.4050, -0.4671,\n",
       "         -0.3421, -0.2254, -0.8824, -0.1009,  0.0233,  0.2069,  0.1794,  0.3239,\n",
       "         -0.0964, -0.4686,  0.3810,  0.2076, -0.3138, -0.7911,  0.8461, -0.7202,\n",
       "         -0.4349, -0.2346,  0.2357, -0.1445,  0.5367, -0.1352, -0.3690,  0.3524,\n",
       "          0.3912, -0.2017, -0.6177, -0.5903, -0.4719, -0.7227, -0.3008,  0.6055,\n",
       "         -0.5498, -0.0379,  0.2380, -0.0262,  0.1003, -0.5817,  0.0301, -0.7017,\n",
       "         -0.3431, -0.5878, -0.5118, -0.3543,  0.1893, -0.3429, -0.5714,  0.0876,\n",
       "          0.1194, -0.1359, -0.1843, -0.0078,  0.0453,  0.0091,  0.3840,  0.3942,\n",
       "          0.0881, -0.3347, -0.6192,  0.5320, -0.3981, -0.2215,  0.0288, -0.7018,\n",
       "         -0.3936, -0.1664, -0.9113,  0.0239, -0.8463, -0.4414, -0.2876, -0.2067,\n",
       "         -0.3295, -0.2154,  0.1052, -0.3279, -0.5225, -0.2852,  0.4161, -0.0444,\n",
       "          0.2454,  0.4289, -0.2671, -0.0353, -0.0752,  0.1722, -0.3804,  0.0154,\n",
       "         -0.3345, -0.4795, -0.5179, -0.5443, -0.3984,  0.7265, -0.1945,  0.1218,\n",
       "          0.1668, -0.4059, -0.5782,  0.0780, -0.0881,  0.0222, -0.3610, -0.3584,\n",
       "         -0.6861, -0.1538,  0.1578, -0.3736, -0.6473,  0.4115,  0.4886,  0.1840,\n",
       "         -0.5478, -0.4983,  0.2972,  0.5690,  0.3309, -0.2842, -0.3175, -0.2666,\n",
       "         -0.9999,  0.2734, -0.1006, -0.1617, -0.7430, -0.1625, -0.5546,  0.4435,\n",
       "         -0.0842, -0.7197, -0.4903, -0.0926,  0.5341,  0.1004,  0.4069,  0.2974,\n",
       "          0.3224,  0.6898, -0.4209, -0.5051,  0.3306,  0.6172,  0.0448, -0.0720,\n",
       "         -0.6987, -0.3116, -0.6661, -0.2449, -0.1565,  0.4975, -0.3346, -0.3259,\n",
       "          0.0861, -0.2086, -0.5540, -0.6837,  0.5036,  0.1201, -0.3571, -0.8150,\n",
       "         -0.4453, -0.4283,  0.1214, -0.6425,  0.2511, -0.0730, -0.1013,  0.3908,\n",
       "          0.3457,  0.2251,  0.3356,  0.6278, -0.2927,  0.2347, -0.2706, -0.2922,\n",
       "         -0.5287, -0.1098,  0.2264,  0.0417, -0.1739, -0.4119, -0.7789, -0.1971,\n",
       "          0.3910,  0.1549,  0.2627,  0.4425,  0.2480, -0.0088,  0.4077, -0.4723,\n",
       "         -0.2160, -0.3966,  0.1869, -0.9176, -0.2266,  0.4312,  0.3206, -0.0213,\n",
       "         -0.3782, -0.2323, -0.7232, -0.0210,  0.0175,  0.2668,  0.2081, -0.1316,\n",
       "          0.4953, -0.6938, -0.3616, -0.6000,  0.3122, -0.0862,  0.2187, -0.2241,\n",
       "         -0.3908, -0.0121, -0.0845, -0.2833, -0.1566,  0.6334,  0.3422, -0.5163,\n",
       "          0.2353,  0.4765, -0.1812, -0.0126,  0.0586,  0.0777,  0.2765, -0.6453,\n",
       "          0.1985,  0.2655,  0.1590,  0.5392, -0.4698,  0.5061, -0.0122, -0.4768,\n",
       "         -0.3566,  0.1313,  0.6343,  0.6204,  0.0632, -0.4949,  0.0634, -0.1299,\n",
       "          0.6617, -0.4003, -0.6039, -0.1597,  0.0774, -0.8556, -0.1800, -0.4191,\n",
       "         -0.0684, -0.6686, -0.6705,  0.0225, -0.7428, -0.4386, -0.1873, -0.0242,\n",
       "          0.3584,  0.5368,  0.1846,  0.0523, -0.3501, -0.0486, -0.8947, -0.8927,\n",
       "          0.1559, -0.0528, -0.3330, -0.1591,  0.1767,  0.2471, -0.2279, -0.2076,\n",
       "         -0.6723,  0.5211,  0.0225, -0.1461,  0.5210, -0.8536, -0.9083, -0.0799,\n",
       "         -0.7456, -0.3022, -0.0733,  0.0375, -0.5943, -0.4184,  0.0317,  0.0709,\n",
       "         -0.5830,  0.1400,  0.2955,  0.3589,  0.0328,  0.0860, -0.1874,  0.2691,\n",
       "          0.5289, -0.2770, -0.3766, -0.2006, -0.1713, -0.1120, -0.1904, -0.0041,\n",
       "          0.0060, -0.6671,  0.0905, -0.2290,  0.3815,  0.5903,  0.6071, -0.6952,\n",
       "         -0.2242, -0.6872, -0.2317,  0.1446, -0.4258, -0.4252, -0.4839, -0.0350,\n",
       "         -0.1800, -0.0641, -0.0816, -0.0076, -0.3388,  0.2529, -0.2564, -0.6264,\n",
       "          0.5584, -0.6245, -0.0238, -0.5147, -0.1438, -0.2713, -0.0776,  0.6896,\n",
       "         -0.2156, -0.3735,  0.3229, -0.1718,  0.1027,  0.1200, -0.3572, -0.2890,\n",
       "         -0.0829, -0.2997, -0.7472, -0.8631, -0.6254,  0.5403, -0.4530,  0.1580,\n",
       "         -0.1012, -0.5739, -0.0975, -0.2738, -0.5252, -0.2313,  0.0262,  0.0888,\n",
       "         -0.1370, -0.2378, -0.9258,  0.0396,  0.4126, -0.5276,  0.1297, -0.1658,\n",
       "         -0.6331,  0.2846,  0.0874, -0.4939, -0.3545,  0.4597, -0.3932, -0.8452,\n",
       "          0.1496,  0.0882, -0.8608, -0.4054,  0.5292, -0.1500, -0.0826, -0.9362,\n",
       "         -0.3869,  0.3060,  0.5733, -0.2123, -0.2426,  0.3264, -0.5594, -0.4495,\n",
       "         -0.1567, -0.8157, -0.5030, -0.3820, -0.7159,  0.0289, -0.5007,  0.5047,\n",
       "         -0.3185, -0.0335,  0.1817,  0.0208, -0.0114, -0.4612, -0.5722, -0.6119,\n",
       "         -0.8171, -0.3889,  0.1450,  0.3049, -0.0795, -0.0207,  0.2374, -0.6992,\n",
       "          0.0375,  0.0357, -0.2678, -0.3664, -0.4512, -0.5232,  0.1674, -0.5599,\n",
       "         -0.2143,  0.4318, -0.6552, -0.3711,  0.4790,  0.1683, -0.2359, -0.2002,\n",
       "          0.0984,  0.2759, -0.5927, -0.3431, -0.6578, -0.2393, -0.3103, -0.2330,\n",
       "          0.0991, -0.6124,  0.0852,  0.4809, -0.3507, -0.5402,  0.2130,  0.4834,\n",
       "         -0.0053, -0.3779,  0.3591,  0.1567,  0.7176,  0.2493, -0.3612, -0.2005,\n",
       "         -0.1553, -0.7255, -0.0781, -0.6773,  0.1755, -0.3877, -0.7512, -0.5026,\n",
       "         -0.6702, -0.8679, -0.1516,  0.2007, -0.2438, -0.3516,  0.0667, -0.4752,\n",
       "          0.4965, -0.1317,  0.5229,  0.2224,  0.4289, -0.2679, -0.3411, -0.3836,\n",
       "         -0.1464,  0.4939,  0.2150,  0.6419,  0.3451, -0.6865, -0.3397,  0.6261,\n",
       "          0.0198, -0.2721, -0.1991, -0.6064,  0.0922, -0.5832,  0.4656,  0.0736,\n",
       "         -0.3208, -0.4758,  0.2843, -0.6126, -0.4163, -0.4805, -0.4276,  0.2367,\n",
       "          0.1017, -0.3112, -0.6239,  0.2198, -0.0818,  0.0361, -0.5453,  0.0706,\n",
       "         -0.5256, -0.4365, -0.0453,  0.1992,  0.2520,  0.0768,  0.3250, -0.7681,\n",
       "         -0.2799,  0.4042, -0.0101, -0.3493,  0.0636,  0.2232, -0.0988,  0.2529,\n",
       "         -0.5888,  0.4130,  0.5940, -0.7914, -0.1917, -0.4807, -0.4094, -0.2444,\n",
       "         -0.3852, -0.3398, -0.4136, -0.5778,  0.5845, -0.7191, -0.2622, -0.6843,\n",
       "          0.4278,  0.3362,  0.4608, -0.5559,  0.5230, -0.0783,  0.3351,  0.0152,\n",
       "         -0.1446,  0.1063,  0.2334,  0.0119, -0.0919,  0.1184,  0.2128, -0.9046,\n",
       "         -0.9100, -0.8075, -0.3156, -0.7010, -0.4280, -0.5739,  0.5602,  0.0601,\n",
       "          0.5187,  0.1778, -0.5707,  0.0930, -0.3568, -0.3595, -0.0842, -0.1316,\n",
       "         -0.7067,  0.0292, -0.1369,  0.6357, -0.0132, -0.4559, -0.0657, -0.2112,\n",
       "         -0.8355,  0.2632,  0.7126, -0.3906,  0.0548, -0.9432, -0.6568,  0.0815,\n",
       "         -0.6400,  0.1598, -0.4667, -0.6195,  0.0661,  0.2923, -0.3094, -0.8176,\n",
       "         -0.8798, -0.7036,  0.7564,  0.5706,  0.2711, -0.5834,  0.5153,  0.5993,\n",
       "          0.4448,  0.5234,  0.1437, -0.3876, -0.7977, -0.0030,  0.8364, -0.2085,\n",
       "         -0.6998, -0.6893, -0.2711, -0.4600,  0.1734,  0.2169,  0.1277,  0.3778,\n",
       "         -0.1573,  0.0429, -0.4235,  0.0561,  0.0400, -0.6303, -0.9571, -0.6005,\n",
       "         -0.9373, -0.4196, -0.5169, -0.6636,  0.2426, -0.8927, -0.5726, -0.6355,\n",
       "          0.1982, -0.0308,  0.1931, -0.8101,  0.1963, -0.4566, -0.5819, -0.3223,\n",
       "         -0.2876,  0.3220,  0.1160, -0.1744, -0.2318, -0.0076, -0.0055, -0.1205,\n",
       "          0.7508, -0.2816,  0.4392, -0.3948,  0.1447, -0.0912,  0.1238, -0.1917]),\n",
       " tensor([ 0.4266, -0.0935,  0.3129, -0.9575, -0.4136,  0.6255,  0.4712, -0.7031,\n",
       "         -0.5912,  0.9226,  0.3512,  0.7238,  0.1275, -0.6040, -0.8056,  0.1048,\n",
       "          0.2601, -0.9428,  0.0268,  0.3240,  0.1884, -0.4723,  0.0102,  0.0082,\n",
       "         -0.9816,  0.1301, -0.4909, -0.8483, -0.1423, -0.3387, -0.2053,  0.2372,\n",
       "          0.7417, -0.3362, -0.7656,  0.4067, -0.7086, -0.6138,  0.5865,  0.4263,\n",
       "         -0.1535, -0.4369, -0.5884,  0.3916, -0.6399, -0.4820,  0.1017,  0.2921,\n",
       "         -0.0608, -0.8167, -0.7153, -0.9322, -0.2517,  0.7085, -0.8998,  0.6177,\n",
       "         -0.2512, -0.3931, -0.8844, -0.8841, -0.6595, -0.2593,  0.1280,  0.5719,\n",
       "         -0.0542, -0.4638,  0.1918,  0.1676, -0.4721, -0.8592,  0.3594, -0.5390,\n",
       "         -0.9205,  0.8595, -0.8588, -0.2025, -0.2036, -0.4622, -0.5388, -0.5316,\n",
       "          0.9026, -0.0675, -0.8290, -0.0191, -0.7022, -0.4186, -0.7168,  0.1711,\n",
       "          0.5348, -0.9082, -0.9450,  0.1821, -0.4750, -0.5867, -0.0167, -0.5659,\n",
       "         -0.1943, -0.8448, -0.4472, -0.7293, -0.9740, -0.7620, -0.1416, -0.8260,\n",
       "          0.2152,  0.1869,  0.8569, -0.5597,  0.1153,  0.5110,  0.5646, -0.7042,\n",
       "         -0.4780, -0.1131, -0.1604, -0.0676, -0.8125, -0.9851, -0.5311,  0.4490,\n",
       "          0.1102,  0.0603, -0.9868, -0.8676, -0.2429,  0.6970, -0.1714,  0.5015,\n",
       "         -0.1673, -0.9904,  0.4450, -0.8132, -0.2831, -0.7639, -0.4467,  0.2265,\n",
       "         -0.9527,  0.5213, -0.8820,  0.4729,  0.0138, -0.2022,  0.0616,  0.0502,\n",
       "         -0.9365, -0.8848, -0.9095, -0.2315,  0.1482,  0.0143, -0.6463, -0.1502,\n",
       "          0.3818, -0.4566, -0.0759, -0.2228, -0.3664, -0.7488,  0.6749,  0.0954,\n",
       "          0.0459, -0.2550, -0.9041,  0.5864, -0.5660,  0.2106, -0.5548, -0.3877,\n",
       "         -0.2353, -0.6701, -0.4686, -0.6815, -0.7800, -0.5742,  0.2504, -0.5364,\n",
       "         -0.0855, -0.7787, -0.5764, -0.2398,  0.2959, -0.6263,  0.0744, -0.1637,\n",
       "         -0.8222,  0.5522, -0.5764, -0.9241, -0.9118,  0.7914, -0.1613,  0.6744,\n",
       "          0.2478,  0.5161, -0.4472, -0.4524, -0.6731, -0.8601, -0.2221, -0.0982,\n",
       "         -0.0919,  0.2172,  0.7111,  0.6249,  0.1133, -0.2514,  0.7841, -0.8608,\n",
       "         -0.2408,  0.2349, -0.5607,  0.2755,  0.8918,  0.2393, -0.9356, -0.0562,\n",
       "         -0.9840,  0.4858,  0.8452, -0.8741, -0.7724, -0.8686, -0.2832, -0.7535,\n",
       "         -0.4181, -0.4337, -0.4486, -0.3472, -0.6706, -0.4887, -0.2399, -0.1347,\n",
       "         -0.8124, -0.8618, -0.7301,  0.2403, -0.3883,  0.0874, -0.4585, -0.7866,\n",
       "          0.3205, -0.8681, -0.3018, -0.6344, -0.4852,  0.6843, -0.7327, -0.4071,\n",
       "         -0.3053, -0.4614, -0.8020, -0.0935, -0.9340,  0.5588, -0.4589, -0.7512,\n",
       "         -0.8186, -0.7220, -0.5550,  0.2052,  0.6309, -0.8161, -0.4215,  0.3473,\n",
       "         -0.5359, -0.6740,  0.5574,  0.7241, -0.8039, -0.6186,  0.3307, -0.8448,\n",
       "         -0.3074, -0.6697,  0.6689, -0.9915, -0.5809, -0.2468, -0.8674,  0.3402,\n",
       "         -0.5868,  0.4627, -0.3159, -0.3643,  0.4553, -0.1884,  0.7083, -0.7475,\n",
       "         -0.0303, -0.0839, -0.3080,  0.1939, -0.0720, -0.4177, -0.4242, -0.8624,\n",
       "         -0.6516, -0.8793, -0.7112,  0.4100,  0.5683, -0.8449, -0.8618, -0.7616,\n",
       "          0.4204,  0.4593, -0.4947, -0.5574, -0.5212, -0.9125, -0.9572,  0.7752,\n",
       "         -0.0727, -0.1656, -0.5038,  0.3033, -0.9161, -0.7736,  0.4784, -0.2744,\n",
       "          0.3665,  0.2633,  0.3636,  0.1255, -0.3581, -0.6631,  0.6836, -0.0261,\n",
       "          0.3603, -0.4657, -0.2673,  0.3122,  0.8429,  0.7582,  0.3860, -0.3416,\n",
       "         -0.6699, -0.2428,  0.1364,  0.4309, -0.8668, -0.9065, -0.3374, -0.0392,\n",
       "          0.6194, -0.3776, -0.5102, -0.0865,  0.3319, -0.5219, -0.5969, -0.5937,\n",
       "          0.3358, -0.6444, -0.0575, -0.7528, -0.3284,  0.6836,  0.6857, -0.2300,\n",
       "         -0.2182, -0.6809, -0.3639, -0.6624, -0.7677, -0.6244,  0.5739, -0.8619,\n",
       "          0.8237, -0.2575, -0.3017,  0.4961, -0.6088,  0.5427, -0.0813, -0.8723,\n",
       "          0.5609,  0.6895, -0.0738, -0.2737, -0.1932, -0.3462, -0.2684,  0.6316,\n",
       "          0.2368,  0.4908, -0.4148, -0.6943, -0.4462, -0.4412, -0.7960, -0.8066,\n",
       "          0.2080, -0.9178,  0.1440,  0.0842, -0.1243, -0.3743, -0.9187, -0.6516,\n",
       "          0.7230,  0.1684, -0.8725, -0.8504, -0.0962, -0.9424,  0.1732, -0.3220,\n",
       "          0.1731, -0.8466,  0.5249, -0.3123, -0.0237,  0.0078,  0.2433, -0.7897,\n",
       "          0.1247,  0.5048,  0.0988,  0.1541, -0.0860, -0.7820, -0.0216, -0.6944,\n",
       "         -0.8411,  0.0564, -0.3211, -0.6697, -0.2579, -0.2855,  0.8025, -0.6346,\n",
       "          0.3753, -0.4635,  0.2974, -0.2169, -0.8681, -0.9117,  0.1839,  0.4464,\n",
       "          0.1320,  0.5137, -0.9103, -0.6116,  0.0329, -0.8138, -0.6767, -0.9446,\n",
       "         -0.8603,  0.4949, -0.1933, -0.2197,  0.5470, -0.8137,  0.2885,  0.0827,\n",
       "         -0.7385, -0.8459, -0.6691, -0.1430, -0.8898,  0.5757,  0.1963, -0.4780,\n",
       "         -0.9517,  0.1064, -0.7523, -0.9241, -0.8975, -0.8256, -0.2161, -0.5770,\n",
       "         -0.4667,  0.1455, -0.0560, -0.6416, -0.6627, -0.8668, -0.9259, -0.2663,\n",
       "          0.2065, -0.7834, -0.6666, -0.8682,  0.0932, -0.7108, -0.2877, -0.5412,\n",
       "         -0.4581, -0.2052, -0.1334,  0.2452, -0.3968, -0.7067, -0.8692, -0.1382,\n",
       "         -0.1010, -0.5349, -0.9101, -0.8843, -0.4551, -0.9604, -0.6502, -0.0955,\n",
       "         -0.3394, -0.4749,  0.1631,  0.0855,  0.2618, -0.7466, -0.6196,  0.1560,\n",
       "         -0.9087, -0.3850,  0.3134, -0.5705, -0.3760, -0.7925, -0.1856,  0.1331,\n",
       "          0.3855,  0.7236, -0.7180, -0.7905, -0.0207, -0.3978, -0.0230, -0.7122,\n",
       "         -0.4523, -0.4833,  0.1252, -0.9471, -0.3717, -0.3667,  0.7397,  0.1812,\n",
       "         -0.6514, -0.2936, -0.3446, -0.4750, -0.5151, -0.5390,  0.1458, -0.9908,\n",
       "         -0.2658, -0.5149, -0.8060,  0.0884, -0.2210, -0.7317,  0.2820, -0.3775,\n",
       "         -0.2562,  0.3692,  0.3393, -0.8954, -0.4012, -0.8191,  0.7579, -0.3699,\n",
       "          0.2412, -0.5447, -0.3969, -0.4908, -0.7033, -0.3207, -0.4022,  0.0427,\n",
       "         -0.4003, -0.7645, -0.8838,  0.0549, -0.5616, -0.0071,  0.5998, -0.5677,\n",
       "         -0.8643, -0.3566, -0.6089, -0.6835, -0.8776, -0.4289, -0.0491, -0.0239,\n",
       "         -0.8063,  0.1928, -0.2467, -0.9705, -0.5924, -0.9263, -0.9587, -0.1897,\n",
       "         -0.4053, -0.5333, -0.2538, -0.7767,  0.7137, -0.3770,  0.7722,  0.7341,\n",
       "         -0.2411, -0.2863, -0.5092,  0.2177,  0.8163, -0.3330,  0.2107, -0.2786,\n",
       "         -0.8107, -0.8139, -0.8487, -0.6402, -0.8896, -0.7091, -0.9928, -0.1010,\n",
       "         -0.7225, -0.4797, -0.2565,  0.3471, -0.1312, -0.5410,  0.6007, -0.5215,\n",
       "         -0.6818,  0.1674, -0.4742, -0.8799,  0.3105, -0.3344, -0.3118,  0.1257,\n",
       "         -0.5594, -0.0134, -0.3605, -0.9032, -0.5389,  0.0267, -0.7486, -0.9789,\n",
       "         -0.4218, -0.2525, -0.9880,  0.1721, -0.5774, -0.4524, -0.6745, -0.0765,\n",
       "         -0.3586, -0.3109, -0.2438, -0.6645,  0.7494, -0.9228,  0.9094,  0.3870,\n",
       "         -0.3960, -0.5521,  0.0690,  0.9584, -0.0511,  0.7952, -0.2895, -0.5383,\n",
       "         -0.9176,  0.4754, -0.0200, -0.7782, -0.5798, -0.5133,  0.6066, -0.6058,\n",
       "         -0.7887, -0.9260, -0.2580,  0.3503,  0.9559, -0.2084,  0.2410, -0.4720,\n",
       "         -0.4020, -0.6500,  0.0855,  0.6801,  0.5475, -0.9275,  0.8384, -0.7334,\n",
       "         -0.6207, -0.2083, -0.0423, -0.5882,  0.2627, -0.6549, -0.8877, -0.1566,\n",
       "         -0.8570, -0.5177, -0.6137, -0.9727, -0.7682, -0.6183, -0.9622,  0.5222,\n",
       "         -0.9893, -0.8395,  0.5856, -0.8400, -0.3215, -0.1220, -0.2072,  0.6140,\n",
       "          0.1084, -0.2783, -0.8522, -0.2588, -0.4265, -0.4752,  0.5383,  0.1244,\n",
       "         -0.5414, -0.0776,  0.6176,  0.3858, -0.4595, -0.5769, -0.2455,  0.2482,\n",
       "          0.1750,  0.0763, -0.6231,  0.2893, -0.5107, -0.9267, -0.2629, -0.6079,\n",
       "          0.0098,  0.2578, -0.9438, -0.9921, -0.8460, -0.5135, -0.7174, -0.6054,\n",
       "          0.5057, -0.0688, -0.8323, -0.5839, -0.8142, -0.8290, -0.6038, -0.8826,\n",
       "         -0.7883, -0.7770,  0.0474,  0.2876, -0.2798,  0.2980, -0.5909, -0.7427,\n",
       "          0.6472, -0.8397,  0.3287, -0.7363, -0.9182, -0.0335,  0.0573, -0.5042]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_outputs[1][0], english_outputs[1][1], english_outputs[1][2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
