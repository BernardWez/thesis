{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "Apply out-of-the-box NER model from [flair](https://github.com/flairNLP/flair) on three datasets that are commonly used for geoparsing: [LGL](https://github.com/geoai-lab/EUPEG/blob/master/corpora/lgl.xml), [GeoWebNews](https://github.com/geoai-lab/EUPEG/blob/master/corpora/GWN.xml), and [TR-News](https://github.com/geoai-lab/EUPEG/blob/master/corpora/TR-News.xml).\n",
    "\n",
    "Goal: Compare geotagging scores (i.e. ability to recognize locations) between a state-of-the-art NER model compared to existing geoparsers systems.\n",
    "\n",
    "Metrics: Recall, precision, and f-score\n",
    "\n",
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TR-News results\n",
    "\n",
    "|   Geoparser Name  | Precision | Recall | F-Score |\n",
    "|-------------------|-----------|--------|---------|\n",
    "| StanfordNERparser |   0.890   |  0.731 |  0.803  |\n",
    "| TopoClusterparser |   0.883   |  0.714 |  0.790  |\n",
    "|__Flair__         |__0.803__  |__0.699__|__0.748__|\n",
    "|   CamCoderparser  |   0.897   |  0.638 |  0.746  |\n",
    "|   DBpediaparser   |   0.861   |  0.631 |  0.728  |\n",
    "|    CLAVINparser   |   0.908   |  0.505 |  0.649  |\n",
    "|  Edinburghparser  |   0.709   |  0.538 |  0.612  |\n",
    "|   SpaCyNERparser  |   0.659   |  0.402 |  0.500  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LGL results\n",
    "\n",
    "\n",
    "| Geoparser Name    | Precision | Recall | F-Score |\n",
    "|-------------------|-----------|--------|---------|\n",
    "|__Flair__          |__0.763__  |__0.676__|__0.717__|\n",
    "| DBpediaparser     | 0.813     | 0.635  | 0.713   |\n",
    "| StanfordNERparser | 0.744     | 0.622  | 0.677   |\n",
    "| TopoClusterparser | 0.763     | 0.577  | 0.657   |\n",
    "| CamCoderparser    | 0.811     | 0.548  | 0.654   |\n",
    "| CLAVINparser      | 0.808     | 0.444  | 0.573   |\n",
    "| Edinburghparser   | 0.723     | 0.383  | 0.501   |\n",
    "| SpaCyNERparser    | 0.493     | 0.371  | 0.423   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GeoWebNews results\n",
    "\n",
    "|   Geoparser Name  | Precision | Recall | F-Score |\n",
    "|-------------------|-----------|--------|---------|\n",
    "|__Flair__          |__0.901__  |__0.662__|__0.763__|\n",
    "| StanfordNERparser |   0.885   |  0.635 |  0.739  |\n",
    "|   CamCoderparser  |   0.895   |  0.562 |  0.691  |\n",
    "| TopoClusterparser |   0.838   |  0.559 |  0.670  |\n",
    "|  Edinburghparser  |   0.819   |  0.538 |  0.650  |\n",
    "|   DBpediaparser   |   0.847   |  0.510 |  0.637  |\n",
    "|    CLAVINparser   |   0.909   |  0.394 |  0.549  |\n",
    "|   SpaCyNERparser  |   0.561   |  0.389 |  0.460  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges\n",
    "\n",
    "- __Assigning ORG labels to toponyms__\n",
    "    \n",
    "    There are quite a lot of occurances where the flair model assigns an ORG label to a toponym leading to more false negatives (i.e. lower recall)\n",
    "    \n",
    "    _Example_: `Anderson again urged all [...] to contact the Avoyelles Parish Sheriffâ€™s Office immediately [...].` \n",
    "    \n",
    "    The dataset labels `Avoyelles Parish` as a toponym. \n",
    "    \n",
    "    However, the flair model predicts `Avoyelles Parish Sheriff's Office` to be of type ORG rather than LOC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Associative Toponyms__\n",
    "    \n",
    "    _Example_: `But both the Russian and Turkish administrations have the determination not to fall for this provocation.`\n",
    "    \n",
    "    The dataset labels `Russian` and `Turkish` as toponyms.\n",
    "    \n",
    "    Flair model predicts these entities as type MISC rather than LOC.\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __GeoWebNews contains many toponyms that appear non-location related__\n",
    "    \n",
    "    _Examples_: `A door opens to a brick-paved outdoor terrace and fenced back yard.`\n",
    "    \n",
    "    `terrace` and `yard` are listed as toponyms\n",
    "    \n",
    "    To address this I ignore all toponyms that do not contain longitude and/or latitude coordiantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Inconsistent labels__\n",
    "    \n",
    "    _Example_: `White House` is considered a toponym for one article, but not for another in the TR-News dataset.\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "- Considering performance other out-of-the-box NER models (working on NER models provided by Huggingface)\n",
    "- Deteremine how to deal with:\n",
    "    1. ORG labels that are predicted by NER models (--> these entities are often associated with some sort of location)\n",
    "    2. Associate toponyms (e.g. is it desired to have a model connect `Russian` to `Russia` or not?)\n",
    "- Evaluation --> reference literature to get better idea how they deal with inconsistencies / evaluate these datasets to make sure approaches are comparable\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
