{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import xml.etree.ElementTree as et"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def calc_precision(tp, fp):\n",
    "    return tp/(tp + fp)\n",
    "\n",
    "def calc_recall(tp, fn):\n",
    "    return tp/(tp + fn)\n",
    "\n",
    "def calc_fscore(precision, recall):\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "def evaluate(gold_truth_labels, predictions):\n",
    "    # Counts of true positives, false positives & false negatives\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    \n",
    "    # List with false positives and false negatives\n",
    "    fps, fns = [], []\n",
    "    \n",
    "    for gold, pred in zip(gold_truth_labels, predictions):\n",
    "        \n",
    "        tp_tmp, fp_tmp, fn_tmp, fns_temp, fps_temp  = evaluate_one_article(gold, pred)\n",
    "        \n",
    "        tp += tp_tmp\n",
    "        fp += fp_tmp\n",
    "        fn += fn_tmp\n",
    "        \n",
    "        fns.extend(fns_temp)\n",
    "        fps.extend(fps_temp) \n",
    "        \n",
    "    precision = calc_precision(tp, fp)\n",
    "    recall = calc_recall(tp, fn)\n",
    "    f_score = calc_fscore(precision, recall)    \n",
    "    \n",
    "    print(f'fp: {fp} | tp: {tp} | fn: {fn}')\n",
    "    print(f'precision: {precision} | recall: {recall} | f-score: {f_score}')\n",
    "    \n",
    "    return fps, fns  \n",
    "    \n",
    "\n",
    "def evaluate_one_article(gold_truth, prediction):\n",
    "    \n",
    "    gold = gold_truth['entities'].copy()\n",
    "    pred = prediction['entities'].copy()\n",
    "    \n",
    "    # Counts of true positives, false positives & false negatives\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    \n",
    "    # List with false positives and false negatives\n",
    "    fps, fns = [], []\n",
    "    \n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    while len(gold) > 0 and len(pred) > 0:\n",
    "        i += 1\n",
    "\n",
    "        # Check if the first two elements are the same\n",
    "        if gold[0] == pred[0]:\n",
    "            tp += 1\n",
    "            gold.pop(0)\n",
    "            pred.pop(0)\n",
    "        \n",
    "        else:\n",
    "            # Grab the first appearing element\n",
    "            element, source = (gold[0], 'gold') if gold[0]['start_pos'] < pred[0]['start_pos'] else (pred[0], 'pred')\n",
    "            \n",
    "            # Remove the element first appearing element\n",
    "            if source == 'gold':\n",
    "                fn += 1\n",
    "                fns.append(element['text'])\n",
    "                gold.remove(element)\n",
    "            elif source == 'pred':\n",
    "                fp += 1\n",
    "                fps.append(element['text'])\n",
    "                pred.remove(element)\n",
    "    \n",
    "    if len(gold) > 0:\n",
    "        fn += 1\n",
    "    elif len(pred) > 0:\n",
    "        fp += 1\n",
    "        \n",
    "    return tp, fp, fn, fns, fps       \n",
    "\n",
    "def run_flair(text):\n",
    "\n",
    "    # make a sentence\n",
    "    sentence = Sentence(text)\n",
    "\n",
    "    # run NER over sentence\n",
    "    tagger.predict(sentence)\n",
    "    \n",
    "    for entity in sentence.to_dict(tag_type='ner')['entities']:\n",
    "        print(entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the ner-multi model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "# load tagger\n",
    "tagger = SequenceTagger.load(\"flair/ner-multi\")\n",
    "\n",
    "# make example sentence in any of the four languages\n",
    "sentence = Sentence(\"George Washington ging nach Washington\")\n",
    "\n",
    "# predict NER tags\n",
    "tagger.predict(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TR-News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Policeman shot dead after assassinating Russian ambassador to Turkey, shouting ‘Don’t forget Aleppo!’\\n        '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get file path LGL dataset\n",
    "file_path = '../../data/TR-News/TR-News.xml'\n",
    "\n",
    "# Load the data\n",
    "tree = et.parse(file_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "# Grab example title\n",
    "example = root[0][0].text\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ground truth labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ground_truth = []\n",
    "\n",
    "for article in root:\n",
    "    \n",
    "    gold_truth = {'text': article.find('text').text,\n",
    "                  'entities': sorted([{'text': top.find('phrase').text,\n",
    "                                'start_pos': int(top.find('start').text),\n",
    "                                'end_pos': int(top.find('end').text)} for top in article.findall('toponyms/toponym')\n",
    "                                 if top.find('gaztag/lat') != None and top.find('gaztag/lon') != None\n",
    "                                     ], key=lambda k: k['start_pos'])}\n",
    "    \n",
    "    \n",
    "    all_ground_truth.append(gold_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions for TR-News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 118/118 [01:28<00:00,  1.33it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "\n",
    "for article in tqdm(all_ground_truth):\n",
    "    \n",
    "    text = article['text']\n",
    "    \n",
    "    # make a sentence\n",
    "    sentence = Sentence(text)\n",
    "    \n",
    "    # run NER over sentence\n",
    "    tagger.predict(sentence)\n",
    "    \n",
    "    pred = sentence.to_dict(tag_type='ner')\n",
    "    pred['entities'] = [entity for entity in pred['entities'] if entity['labels'][0].value == 'LOC']\n",
    "    [entity.pop('labels') for entity in pred['entities']]\n",
    "    pred.pop('labels')\n",
    "    \n",
    "    predictions.append(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results TR-News & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fp: 272 | tp: 866 | fn: 387\n",
      "precision: 0.7609841827768014 | recall: 0.6911412609736632 | f-score: 0.7243831033040569\n"
     ]
    }
   ],
   "source": [
    "# only toponyms w/ lat/long\n",
    "fps, fns = evaluate(all_ground_truth, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fp: 241 | tp: 900 | fn: 396\n",
      "precision: 0.7887817703768624 | recall: 0.6944444444444444 | f-score: 0.7386130488305294\n"
     ]
    }
   ],
   "source": [
    "# all toponyms\n",
    "fps, fns = evaluate(all_ground_truth, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LGL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get file path LGL dataset\n",
    "file_path = '../../data/LGL/LGL.xml'\n",
    "\n",
    "# Load the data\n",
    "tree = et.parse(file_path)\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ground truth labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ground_truth = []\n",
    "\n",
    "for article in root:\n",
    "    \n",
    "    gold_truth = {'text': article.find('text').text,\n",
    "                  'entities': sorted([{'text': top.find('phrase').text,\n",
    "                                'start_pos': int(top.find('start').text),\n",
    "                                'end_pos': int(top.find('end').text)} for top in article.findall('toponyms/toponym')\n",
    "                                              if top.find('gaztag/lat') != None and top.find('gaztag/lon') != None\n",
    "                                     ], key=lambda k: k['start_pos'])}\n",
    "    \n",
    "    \n",
    "    all_ground_truth.append(gold_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions for LGL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 588/588 [07:24<00:00,  1.32it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "\n",
    "for article in tqdm(all_ground_truth):\n",
    "    \n",
    "    text = article['text']\n",
    "    \n",
    "    # make a sentence\n",
    "    sentence = Sentence(text)\n",
    "    \n",
    "    # run NER over sentence\n",
    "    tagger.predict(sentence)\n",
    "    \n",
    "    pred = sentence.to_dict(tag_type='ner')\n",
    "    pred['entities'] = [entity for entity in pred['entities'] if entity['labels'][0].value == 'LOC']\n",
    "    [entity.pop('labels') for entity in pred['entities']]\n",
    "    pred.pop('labels')\n",
    "    \n",
    "    predictions.append(pred)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results LGL & comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fp: 1411 | tp: 2818 | fn: 1541\n",
      "precision: 0.6663513833057461 | recall: 0.6464785501261757 | f-score: 0.656264555193293\n"
     ]
    }
   ],
   "source": [
    "# filter toponyms (only w/ lat & long)\n",
    "fps, fns = evaluate(all_ground_truth, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fp: 1035 | tp: 3298 | fn: 1679\n",
      "precision: 0.7611354719593815 | recall: 0.6626481816355234 | f-score: 0.708485499462943\n"
     ]
    }
   ],
   "source": [
    "# all toponyms\n",
    "fps, fns = evaluate(all_ground_truth, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GeoWebNews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get file path LGL dataset\n",
    "file_path = '../../data/GeoWebNews/GeoWebNews.xml'\n",
    "\n",
    "# Load the data\n",
    "tree = et.parse(file_path)\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ground truth labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ground_truth = []\n",
    "\n",
    "for article in root:\n",
    "    \n",
    "    gold_truth = {'text': article.find('text').text,\n",
    "                  'entities': sorted([{'text': top.find('extractedName').text,\n",
    "                                'start_pos': int(top.find('start').text),\n",
    "                                'end_pos': int(top.find('end').text)} for top in article.findall('toponyms/toponym')\n",
    "#                                               if top.find('latitude') != None and top.find('longitude') != None\n",
    "                                     ], key=lambda k: k['start_pos'])}\n",
    "    \n",
    "    \n",
    "    all_ground_truth.append(gold_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions GeoWebNews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [03:12<00:00,  1.04it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "\n",
    "for article in tqdm(all_ground_truth):\n",
    "    \n",
    "    text = article['text']\n",
    "    \n",
    "    # make a sentence\n",
    "    sentence = Sentence(text)\n",
    "    \n",
    "    # run NER over sentence\n",
    "    tagger.predict(sentence)\n",
    "    \n",
    "    pred = sentence.to_dict(tag_type='ner')\n",
    "    pred['entities'] = [entity for entity in pred['entities'] if entity['labels'][0].value == 'LOC']\n",
    "    [entity.pop('labels') for entity in pred['entities']]\n",
    "    pred.pop('labels')\n",
    "    \n",
    "    predictions.append(pred)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results GeoWebNews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fp: 181 | tp: 1652 | fn: 876\n",
      "precision: 0.9012547735951991 | recall: 0.6534810126582279 | f-score: 0.7576243980738363\n"
     ]
    }
   ],
   "source": [
    "# only toponyms with long / lat info\n",
    "fps, fns = evaluate(all_ground_truth, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fp: 162 | tp: 1674 | fn: 4121\n",
      "precision: 0.9117647058823529 | recall: 0.288869715271786 | f-score: 0.438736731752064\n"
     ]
    }
   ],
   "source": [
    "# all toponyms --> fn much higher because many annotated toponyms aren't locations (not sure why this is)\n",
    "fps, fns = evaluate(all_ground_truth, predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
