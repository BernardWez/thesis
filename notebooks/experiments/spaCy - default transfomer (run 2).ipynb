{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import xml.etree.ElementTree as et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def calc_precision(tp, fp):\n",
    "    return tp/(tp + fp)\n",
    "\n",
    "def calc_recall(tp, fn):\n",
    "    return tp/(tp + fn)\n",
    "\n",
    "def calc_fscore(precision, recall):\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "def evaluate(gold_truth_labels, predictions):\n",
    "    # Counts of true positives, false positives & false negatives\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    \n",
    "    # List with false positives and false negatives\n",
    "    fps, fns = [], []\n",
    "    \n",
    "    for gold, pred in zip(gold_truth_labels, predictions):\n",
    "        \n",
    "        tp_tmp, fp_tmp, fn_tmp, fns_temp, fps_temp  = evaluate_one_article(gold, pred)\n",
    "        \n",
    "        tp += tp_tmp\n",
    "        fp += fp_tmp\n",
    "        fn += fn_tmp\n",
    "        \n",
    "        fns.extend(fns_temp)\n",
    "        fps.extend(fps_temp) \n",
    "        \n",
    "    precision = calc_precision(tp, fp)\n",
    "    recall = calc_recall(tp, fn)\n",
    "    f_score = calc_fscore(precision, recall)    \n",
    "    \n",
    "    print(f'fp: {fp} | tp: {tp} | fn: {fn}')\n",
    "    print(f'precision: {precision:.3f} | recall: {recall:.3f} | f-score: {f_score:.3f}')\n",
    "    \n",
    "    return fps, fns  \n",
    "    \n",
    "\n",
    "def evaluate_one_article(gold_truth, prediction):\n",
    "    \n",
    "    gold = gold_truth['entities'].copy()\n",
    "    pred = prediction['entities'].copy()\n",
    "    \n",
    "    # Counts of true positives, false positives & false negatives\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    \n",
    "    # List with false positives and false negatives\n",
    "    fps, fns = [], []\n",
    "    \n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    while len(gold) > 0 and len(pred) > 0:\n",
    "        i += 1\n",
    "\n",
    "        # Check if the first two elements are the same\n",
    "        if gold[0] == pred[0]:\n",
    "            tp += 1\n",
    "            gold.pop(0)\n",
    "            pred.pop(0)\n",
    "        \n",
    "        else:\n",
    "            # Grab the first appearing element\n",
    "            element, source = (gold[0], 'gold') if gold[0]['start_pos'] < pred[0]['start_pos'] else (pred[0], 'pred')\n",
    "            \n",
    "            # Remove the element first appearing element\n",
    "            if source == 'gold':\n",
    "                fn += 1\n",
    "                fns.append(element['text'])\n",
    "                gold.remove(element)\n",
    "            elif source == 'pred':\n",
    "                fp += 1\n",
    "                fps.append(element['text'])\n",
    "                pred.remove(element)\n",
    "    \n",
    "    if len(gold) > 0:\n",
    "        fn += 1\n",
    "    elif len(pred) > 0:\n",
    "        fp += 1\n",
    "        \n",
    "    return tp, fp, fn, fns, fps       \n",
    "\n",
    "def run_flair(text):\n",
    "\n",
    "    # make a sentence\n",
    "    sentence = Sentence(text)\n",
    "\n",
    "    # run NER over sentence\n",
    "    tagger.predict(sentence)\n",
    "    \n",
    "    for entity in sentence.to_dict(tag_type='ner')['entities']:\n",
    "        print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(file_path):\n",
    "    \"\"\"\n",
    "    Loads file and returns all the articles\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    tree = et.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    return root\n",
    "\n",
    "def process_article(article, filtered, file_path):\n",
    "    \"\"\"\n",
    "    Takes article and process into desired structure\n",
    "    \"\"\"\n",
    "    if 'GeoWebNews' in file_path:\n",
    "        if filtered:\n",
    "            return {'text': article.find('text').text, \n",
    "                    'entities': sorted([{'text': top.find('extractedName').text, \n",
    "                                                      'start_pos': int(top.find('start').text), \n",
    "                                                      'end_pos': int(top.find('end').text)} for top in article.findall('toponyms/toponym') \n",
    "                                                     if top.find('latitude') != None and top.find('longitude') != None], key=lambda k: k['start_pos'])}\n",
    "        \n",
    "        else:\n",
    "            return {'text': article.find('text').text, \n",
    "                    'entities': sorted([{'text': top.find('extractedName').text, \n",
    "                                                      'start_pos': int(top.find('start').text), \n",
    "                                                      'end_pos': int(top.find('end').text)} for top in article.findall('toponyms/toponym')], key=lambda k: k['start_pos'])}\n",
    "    \n",
    "    \n",
    "    elif not filtered:\n",
    "        return {'text': article.find('text').text,\n",
    "                'entities': sorted([{'text': top.find('phrase').text,\n",
    "                            'start_pos': int(top.find('start').text),\n",
    "                            'end_pos': int(top.find('end').text)} for top in article.findall('toponyms/toponym')\n",
    "                                 ], key=lambda k: k['start_pos'])}\n",
    "        \n",
    "    else:\n",
    "        return {'text': article.find('text').text,\n",
    "                'entities': sorted([{'text': top.find('phrase').text,\n",
    "                            'start_pos': int(top.find('start').text),\n",
    "                            'end_pos': int(top.find('end').text)} for top in article.findall('toponyms/toponym')\n",
    "                             if top.find('gaztag/lat') != None and top.find('gaztag/lon') != None\n",
    "                                 ], key=lambda k: k['start_pos'])}\n",
    "\n",
    "def process_articles(root, filtered, file_path):\n",
    "    \"\"\"\n",
    "    Takes articles and processes them into desired structure\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for article in root:\n",
    "        \n",
    "        data.append(process_article(article, filtered, file_path))\n",
    "    \n",
    "    return data\n",
    "\n",
    "def prepare_data(file_path, filtered):\n",
    "    \n",
    "    root = load_file(file_path)\n",
    "    \n",
    "    data = process_articles(root, filtered, file_path)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions_spacy(data):\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    texts = [article['text'] for article in data]\n",
    "    \n",
    "    tmp = [sent_tokenize(article) for article in texts]\n",
    "    \n",
    "    for article in tmp:\n",
    "        \n",
    "        pos = 0\n",
    "        \n",
    "        predicted_entities = []\n",
    "        \n",
    "        for sentence in nlp.pipe(article):\n",
    "            \n",
    "            pred = [{'text': ent.text, \n",
    "                     'start_pos': pos + len(sentence[0:ent.end].text) - len(sentence[ent.start]), \n",
    "                     'end_pos': pos + len(sentence[0:ent.end].text)} for ent in sentence.ents if \n",
    "                                                             ent.label_ == 'LOC' or ent.label_ == 'FAC' or ent.label_ == 'GPE']\n",
    "            \n",
    "            predicted_entities.extend(pred)\n",
    "            \n",
    "            pos += len(sentence.text)\n",
    "            \n",
    "            \n",
    "        article_tags = {'text': sentence, 'entities': predicted_entities}\n",
    "\n",
    "        predictions.append(article_tags)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the default transfomer spacy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only enable the ner tagger\n",
    "nlp = spacy.load(\"en_core_web_trf\", disable=[\"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TR-News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get file path LGL dataset\n",
    "file_path = '../../data/TR-News/TR-News.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all_toponyms = prepare_data(file_path, filtered=False)\n",
    "# data_filtered_toponyms = prepare_data(file_path, filtered=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions for TR-News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_all_toponyms = make_predictions_spacy(data_all_toponyms[:2])\n",
    "# predictions_filtered_toponyms = make_predictions_spacy(data_filtered_toponyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'Russia', 'start_pos': 33, 'end_pos': 39},\n",
       " {'text': 'Turkey', 'start_pos': 56, 'end_pos': 62},\n",
       " {'text': 'Russia', 'start_pos': 190, 'end_pos': 196},\n",
       " {'text': 'Syria', 'start_pos': 216, 'end_pos': 221},\n",
       " {'text': 'Aleppo', 'start_pos': 247, 'end_pos': 253},\n",
       " {'text': 'Syria', 'start_pos': 267, 'end_pos': 272},\n",
       " {'text': 'Turkey', 'start_pos': 290, 'end_pos': 296},\n",
       " {'text': 'Russia', 'start_pos': 301, 'end_pos': 307},\n",
       " {'text': 'Ankara', 'start_pos': 327, 'end_pos': 333},\n",
       " {'text': 'Ankara', 'start_pos': 843, 'end_pos': 849},\n",
       " {'text': 'Russia', 'start_pos': 1044, 'end_pos': 1050},\n",
       " {'text': 'Russia', 'start_pos': 1115, 'end_pos': 1121},\n",
       " {'text': 'Aleppo', 'start_pos': 1167, 'end_pos': 1173},\n",
       " {'text': 'Syria', 'start_pos': 1175, 'end_pos': 1180},\n",
       " {'text': 'Aleppo', 'start_pos': 1221, 'end_pos': 1227},\n",
       " {'text': 'Turkey', 'start_pos': 1502, 'end_pos': 1508},\n",
       " {'text': 'U.S.-led', 'start_pos': 1545, 'end_pos': 1553},\n",
       " {'text': 'Syria', 'start_pos': 1658, 'end_pos': 1663},\n",
       " {'text': 'Turkey', 'start_pos': 1738, 'end_pos': 1744},\n",
       " {'text': 'Turkey', 'start_pos': 1791, 'end_pos': 1797},\n",
       " {'text': 'Istanbul', 'start_pos': 1905, 'end_pos': 1913},\n",
       " {'text': 'Syria', 'start_pos': 2258, 'end_pos': 2263},\n",
       " {'text': 'Moscow', 'start_pos': 2278, 'end_pos': 2284},\n",
       " {'text': 'Turkey', 'start_pos': 2348, 'end_pos': 2354},\n",
       " {'text': 'Russia', 'start_pos': 2382, 'end_pos': 2388},\n",
       " {'text': 'Iran', 'start_pos': 2393, 'end_pos': 2397},\n",
       " {'text': 'Russia', 'start_pos': 2517, 'end_pos': 2523},\n",
       " {'text': 'Syria', 'start_pos': 2570, 'end_pos': 2575},\n",
       " {'text': 'Russia', 'start_pos': 2582, 'end_pos': 2588},\n",
       " {'text': 'Turkey', 'start_pos': 2590, 'end_pos': 2596},\n",
       " {'text': 'Iran', 'start_pos': 2601, 'end_pos': 2605},\n",
       " {'text': 'Ankara', 'start_pos': 2765, 'end_pos': 2771},\n",
       " {'text': 'Moscow', 'start_pos': 2982, 'end_pos': 2988},\n",
       " {'text': 'Turkey', 'start_pos': 2999, 'end_pos': 3005},\n",
       " {'text': 'Ankara', 'start_pos': 3053, 'end_pos': 3059},\n",
       " {'text': 'Ankara', 'start_pos': 3406, 'end_pos': 3412},\n",
       " {'text': 'Turkey', 'start_pos': 3811, 'end_pos': 3817},\n",
       " {'text': 'Aydin', 'start_pos': 3912, 'end_pos': 3917},\n",
       " {'text': 'Ankara', 'start_pos': 4050, 'end_pos': 4056},\n",
       " {'text': 'Ankara', 'start_pos': 4241, 'end_pos': 4247},\n",
       " {'text': 'Turkey', 'start_pos': 4555, 'end_pos': 4561},\n",
       " {'text': 'Russia', 'start_pos': 4800, 'end_pos': 4806},\n",
       " {'text': 'Turkey', 'start_pos': 4960, 'end_pos': 4966},\n",
       " {'text': 'Hawaii', 'start_pos': 5191, 'end_pos': 5197},\n",
       " {'text': 'Russia', 'start_pos': 5353, 'end_pos': 5359},\n",
       " {'text': 'Turkey', 'start_pos': 5364, 'end_pos': 5370},\n",
       " {'text': 'Turkey', 'start_pos': 5471, 'end_pos': 5477},\n",
       " {'text': 'Russia', 'start_pos': 5630, 'end_pos': 5636},\n",
       " {'text': 'Turkey', 'start_pos': 5641, 'end_pos': 5647},\n",
       " {'text': 'Aleppo', 'start_pos': 5710, 'end_pos': 5716},\n",
       " {'text': 'Kazakhstan', 'start_pos': 5793, 'end_pos': 5803},\n",
       " {'text': 'Astana', 'start_pos': 5815, 'end_pos': 5821},\n",
       " {'text': 'Turkey', 'start_pos': 5842, 'end_pos': 5848},\n",
       " {'text': 'Turkey', 'start_pos': 5923, 'end_pos': 5929}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_all_toponyms[0]['entities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results TR-News & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only toponyms w/ lat/long\n",
    "fps, fns = evaluate(data_filtered_toponyms, predictions_filtered_toponyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all toponyms\n",
    "fps, fns = evaluate(data_all_toponyms, predictions_all_toponyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### scoring overview\n",
    "\n",
    "|   Geoparser Name                 | Precision | Recall | F-Score |\n",
    "|:-----------------               |:---------:|:------:|:-------:|\n",
    "| StanfordNERparser                     |   0.890   |  0.731 |  0.803  |\n",
    "| TopoClusterparser                     |   0.883   |  0.714 |  0.790  |\n",
    "|__Flair (all toponyms)__               |__0.803__  |__0.699__|__0.748__|\n",
    "|   CamCoderparser                      |   0.897   |  0.638 |  0.746  |\n",
    "|__Flair MULTI ner (all toponyms)__     |__0.779__  |__0.694__|__0.739__|\n",
    "|__Flair MULTI ner FAST (all toponyms)__|__0.802__  |__0.683__|__0.738__|\n",
    "|__Flair (filtered)__                   |__0.773__  |__0.695__|__0.732__|\n",
    "|   DBpediaparser                       |   0.861   |  0.631 |  0.728  |\n",
    "|__Flair MULTI ner (filtered)__         |__0.761__  |__0.691__|__0.724__|\n",
    "|__Flair MULTI ner FAST (filtered)__    |__0.772__  |__0.679__|__0.722__|\n",
    "|    CLAVINparser                       |   0.908   |  0.505 |  0.649  |\n",
    "|  Edinburghparser                      |   0.709   |  0.538 |  0.612  |\n",
    "|__SpaCy MULTI ner (filtered)__         |__0.612__  |__0.561__|__0.586__|\n",
    "|__SpaCy MULTI ner  (all)__             |__0.611__  |__0.542__|__0.575__|\n",
    "|   SpaCyNERparser                      |   0.659   |  0.402 |  0.500  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LGL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get file path LGL dataset\n",
    "file_path = '../../data/LGL/LGL.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all_toponyms = prepare_data(file_path, filtered=False)\n",
    "data_filtered_toponyms = prepare_data(file_path, filtered=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions for LGL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_all_toponyms = make_predictions_spacy(data_all_toponyms)\n",
    "predictions_filtered_toponyms = make_predictions_spacy(data_filtered_toponyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results LGL & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only toponyms w/ lat/long\n",
    "fps, fns = evaluate(data_filtered_toponyms, predictions_filtered_toponyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all toponyms\n",
    "fps, fns = evaluate(data_all_toponyms, predictions_all_toponyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### scoring overview\n",
    " \n",
    "| Geoparser Name                          | Precision | Recall  | F-Score |\n",
    "|:-------------------                     |-----------|-------- |---------|\n",
    "|__Flair (all)__                          |__0.763__  |__0.676__|__0.717__|\n",
    "| DBpediaparser                           | 0.813     | 0.635   | 0.713   |\n",
    "|__Flair MULTI(all)__                     |__0.761__  |__0.663__|__0.708__|\n",
    "|__Flair MULTI FAST(all)__                |__0.770__  |__0.644__|__0.701__|\n",
    "| StanfordNERparser                       | 0.744     | 0.622   | 0.677   |\n",
    "| TopoClusterparser                       | 0.763     | 0.577   | 0.657   |\n",
    "|__Flair (filtered)__                     |__0.660__  |__0.653__|__0.657__|\n",
    "|__Flair MULTI(filtered)__                |__0.666__  |__0.646__|__0.656__|\n",
    "| CamCoderparser                          | 0.811     | 0.548   | 0.654   |\n",
    "|__Flair MULTI FAST (filtered)__          |__0.674__  |__0.629__|__0.651__|\n",
    "| CLAVINparser                            | 0.808     | 0.444   | 0.573   |\n",
    "| Edinburghparser                         | 0.723     | 0.383   | 0.501   |\n",
    "| SpaCyNERparser                          | 0.493     | 0.371   | 0.423   |\n",
    "|__SpaCy MULTI(filtered)__                |__0.415__  |__0.416__|__0.416__|\n",
    "|__SpaCy MULTI FAST (all)__               |__0.419__  |__0.376__|__0.397__|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GeoWebNews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get file path LGL dataset\n",
    "file_path = '../../data/GeoWebNews/GeoWebNews.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all_toponyms = prepare_data(file_path, filtered=False)\n",
    "data_filtered_toponyms = prepare_data(file_path, filtered=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions for GeoWebNews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_all_toponyms = make_predictions_spacy(data_all_toponyms)\n",
    "predictions_filtered_toponyms = make_predictions_spacy(data_filtered_toponyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results GeoWebNews & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only toponyms w/ lat/long\n",
    "fps, fns = evaluate(data_filtered_toponyms, predictions_filtered_toponyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all toponyms\n",
    "fps, fns = evaluate(data_all_toponyms, predictions_all_toponyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### scoring overview\n",
    "|   Geoparser Name                 | Precision | Recall  | F-Score |\n",
    "|:-----------------                |:---------:|:------: |:-------:|\n",
    "|__Flair(filtered)__               |__0.901__  |__0.662__|__0.763__|\n",
    "|__Flair MULTI ner(filtered)__     |__0.901__  |__0.662__|__0.763__|\n",
    "|__Flair MULTI FAST ner(filtered)__|__0.906__  |__0.638__|__0.749__|\n",
    "| StanfordNERparser                |   0.885   |  0.635  |  0.739  |\n",
    "|   CamCoderparser                 |   0.895   |  0.562  |  0.691  |\n",
    "| TopoClusterparser                |   0.838   |  0.559  |  0.670  |\n",
    "|  Edinburghparser                 |   0.819   |  0.538  |  0.650  |\n",
    "|   DBpediaparser                  |   0.847   |  0.510  |  0.637  |\n",
    "|    CLAVINparser                  |   0.909   |  0.394  |  0.549  |\n",
    "|__SpaCy MULTI ner(filtered)__     |__0.564__  |__0.454__|__0.503__|\n",
    "|   SpaCyNERparser                 |   0.561   |  0.389  |  0.460  |\n",
    "|__Flair(all)__                    |__0.911__  |__0.293__|__0.443__|\n",
    "|__Flair MULTI ner(all)__          |__0.912__  |__0.289__|__0.439__|\n",
    "|__Flair MULTI FAST ner(all)__     |__0.912__  |__0.280__|__0.428__|\n",
    "|__SpaCy MULTI ner(all)__          |__0.582__  |__0.200__|__0.298__|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
